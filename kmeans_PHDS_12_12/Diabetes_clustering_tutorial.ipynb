{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Clustering Clinical Data in Python \n\nIn this part of the practical we will be going through step by step how to use k-means on health records to find subtypes of a disease. \n\nIn this tutorial we will use a real health dataset with [data for diabetic patients](https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008) in several US hospitals.\nYou can find a description of the variables via [this link](https://www.hindawi.com/journals/bmri/2014/781670/tab1/)\n\n### Load Packages "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tableone import TableOne\nfrom sklearn.cluster import KMeans\nimport warnings\nimport matplotlib.cm as cm\nfrom sklearn import metrics\nwarnings.filterwarnings('ignore')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Load and Inspect Data "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data = pd.read_csv('diabetic_data.csv')\ndata.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Cleaning Data \nThis data has been cleaned in another script which you can find in this folder. \n<br>\nFirst variables with high missingness were removed, these were weight, max_glu_serum, A1Cresult, payer_code and medical_specialty. \n<br>\nNumber_emergency was removed as over 90% of patients had 0. \n<br>\nThen patients with either no race or gender information were removed (3 rows removed).\n<br>\nAll rows with NA values are then removed (3711 rows removed).\n<br>\nPatients that are under 30 years old are also removed (2009 rows removed).\n<br>\nDuplicated encounters for each patient is removed leaving only the last encounter (28812 rows removed).\n\n### Importing Clean Data "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data_clean = pd.read_csv('diabetic_data_clean.csv')\ndata_clean.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data_clean.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Creating Table One "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# list of columns to be included in tableone\ncolumns = [u'race', u'gender', u'age',\n       u'time_in_hospital', u'num_lab_procedures', u'num_procedures',\n       u'num_medications', u'number_outpatient', \n       u'number_inpatient', u'number_diagnoses', u'metformin',\n       u'glipizide', u'glyburide', u'insulin',\n       u'change', u'diabetesMed', u'readmitted']\n\nnumeric = [u'time_in_hospital', u'num_lab_procedures', u'num_procedures',\n       u'num_medications', u'number_outpatient', \n       u'number_inpatient', u'number_diagnoses']\n\n# list of columns containing categorical variables\ncategorical = [u'race', u'gender', u'age', u'metformin', u'glipizide', u'glyburide', u'insulin', \n               u'change', u'diabetesMed', u'readmitted']\n\ntable_one = TableOne(data_clean, columns, categorical)\ntable_one",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Applying K Means \nTo cluster the data there are the following steps\n<br>\n1) Preparing the data\n<br>\n2) Dimensionality reduction: PCA\n<br>\n3) Deciding cluster number: elbow plot \n<br>\n4) Applying K-Means \n<br>\n5) Characterising clusters \n<br>\n6) Internal validation of clusters\n<br>\n7) External validation of clusters \n\n### Getting the data ready \nFor PCA and K-means the data needs to be numerical, so we are going to select the numerical variables. \n\nWe will also take a sample of the patients as we will run out of memory with a data set this large. \n\nThis numerical data also needs to be scaled as if there variance in one variable is much larger than the others it will result in the principle componant corresponding to that variable. [Here is a nice link that explains this in more detail](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pre_cluster_data = data_clean[numeric] # Selecting only the numeric data ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "np.random.seed(4) # setting random seed so random sample is repeatable \n\nsample_idx = np.random.randint(pre_cluster_data.shape[0],size=10000) \n\npre_cluster_sample = pre_cluster_data.loc[sample_idx,:] # selecting a sample \npre_cluster_sample.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler() # gets z value of variable\n\npre_cluster_scaled = scaler.fit_transform(pre_cluster_sample)\npre_cluster_scaled",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Principle Components Analysis \nNow we will reduce the dimensions, as mentioned in the lecture this is to remove correlation between the variables.\nIn PCA we will find n-1 componants, ordered from the one which explains the most variance, to the least. We want to pick the number of componants that explains ~90% of the variance. \n\nWe will be using the [pca function from scikit learn,](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). One of the parameters it takes is n_components which if greater than or equal to 1, it returns that number of components, or if less that 1 it returns the number of components that explain that proportion of the variance "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.decomposition import PCA\nn_comp =  # Define components/variance returned here \npca = PCA(n_components = n_comp)   \npca_res = pca.fit_transform(pre_cluster_scaled) # the .fit_transform applies PCA to the function and returns the transformed values \npca_res_var = pca.explained_variance_ratio_ # returns the explained variance for each componant ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.DataFrame(pca_res)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.DataFrame(pca_res_var)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can then visualise the fist two components of the data which have the two highest levels of variance. \nWe want to plot the first component on the x axis and the second on the y. The function plt.scatter will create a scatter plot. \nDefine the variables x_values and y_values which will be plotted in the scatter plot "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x_values =  # Define the values for the x coordinates here (the first principle component)\ny_values =  # Define the values for the y coordinates here (the second principle component)\nplt.scatter(x_values, y_values)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Finding Cluster Number \nNow we have our data set which has undergone dimensionality reduction, it is now time to apply K-Means. However before we do we need to find the number of clusters which gives us the best clustering solution. \nAs mentioned in the lecture K-Means tries to minimize the squared error for each point for each cluster. This is also the squared euclidean distance between each point and the center of its corresponding cluster. We can sum this distance up for each point to each cluster centre to get a total for each cluster, then sum this up for each cluster. This, in scikit learn is called inertia. A low inertia means that all the points are very close to their assigned cluster centers and thus a better clustering solution. \nLets run K-Means on our data set with two clusters and take a look at the inertia\n\n[More information on the scikit learn kmeans function](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "k = 2\nkmeans = KMeans(n_clusters= k, init = 'k-means++' , random_state = 0).fit(pca_res)\nkmeans.inertia_    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This value tells us nothing about how good a solution we found unless we compare it to the inertia found for different cluster numbers \nBelow there is a for loop, complete the for loop so it loops a range of values of k (cluster number), runs k means and returns the inertia value. \nYou will have to define the range of different cluster numbers you want to test. inertia_df is an empty data frame with the column names 'k' and 'inertia', you can define what is in a row by using df.loc[x] where x is the row label.  "
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "col_names = [\"k\", \"inertia\"] # Empty inertia array\ninertia_df = pd.DataFrame(columns = col_names)\nfor k in range():# fill in range here\n    # fill in for loop here",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "inertia_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "the list below are the inertia values for the different cluster numbers, however it is hard to determine value of K to use based on this list. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "inertia_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Elbow plot \nTo work out the best value for K we can plot these values on a line graph with cluster number on the x axis and inertia on the y. As the cluster number increases the inertia will decrease, however it will not decrease linearly. The change in the inertia will start decreasing less between two cluster values which will form an \"elbow\" in the line graph (this is why its called an elbow plot). At the point where this elbow forms is what this method thinks is the best cluster number\n\n#### Plotting the elbow graph \nSet values for cluster_N and inertia to plot the elbow plot "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "cluster_N =  # set  values for x coordinates, which is cluster numbers \ninertia =   # set values for y coordinates \nplt.plot(cluster_N, inertia)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "What cluster number do you think the elbow plot suggests should be used? \n<br>\nDo you think this is the best method we could use? \n<br>\nWhat are the problems with it? \n<br>\n\n### Elbow plot with a silhouette score\nWhen K-Means Runs it aims to minimize the inertia,  we are now going to have a look at a metric that K-means does not try to minimize. For this we will use a silhouette score. This is using the mean silhouette coeffecient for each cluster result. The silhouette coefficient is a measure of how much each point belongs to its assigned cluster and it varies between -1 and 1, with values closer to 1 indicating better cluster assignment. \n\nFirst we will look at the silhouette score when k = 2 "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dist = metrics.pairwise_distances(pca_res) # finds a distance matrix using euclidian distance \nk = 2\nkmeans = KMeans(n_clusters= k, init = 'k-means++' , random_state = 0).fit(pca_res)\nsilhouette_score = metrics.silhouette_score(dist, kmeans.labels_ , metric='precomputed') \nsilhouette_score",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now fill in the for loop below to carry it out for a range of k "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "col_names_sil = [\"k\", \"sil_score\"] \nsil_df = pd.DataFrame(columns = col_names_sil) # Empty silhouette dataframe\nfor k in range():# fill in range here\n    # fill in for loop here ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sil_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "cluster_N =  # set  values for x coordinates, which is cluster numbers \nsil_score =  # set values for y coordinates \nplt.plot(cluster_N, sil_score)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Does this give a clearer idea of what number to pick for k ? \n\n\n### Final K Means Results \nTake the value for K you found from the elbow plots and run K means, this will give us the final cluster results "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "final_k =  # fill in your final choice of cluster number here \nfinal_kmeans = KMeans(n_clusters = final_k, init = 'k-means++' , random_state = 0).fit(pca_res)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finding out how many patients in each cluster "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "np.unique(final_kmeans.labels_, return_counts = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Cluster Characterisation\nNow we have assigned each patient to a cluster, we can see what variables that we used are high or low in each cluster."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "post_cluster_data = pre_cluster_sample",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "post_cluster_data['cluster'] = 0\nfor i in range(final_k):\n    post_cluster_data['cluster'][final_kmeans.labels_ == i] = (i + 1)\n\npost_cluster_data.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "kmeans_table_one = TableOne(post_cluster_data, numeric, groupby = 'cluster')\nkmeans_table_one",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Internal Validation measures \n#### PCA Plot \nWe can also visualise our clusters using the pca plot we made earlier "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "LABEL_COLOUR_MAP = {0:'r', 1: 'g', 2: 'b', 3: 'c' , 4 : 'm'} # dictionary for colours \nlabel_colour = [LABEL_COLOUR_MAP[l] for l in final_kmeans.labels_]  ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.scatter(x_values, y_values, c = label_colour, alpha = 0.5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Can you come to any conclusions about the results from the cluster analysis "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Silhouette plot \nHere we create a silhouette plot to visualise how much each point \"belongs\" to its assigned cluster. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "silhouette_kmeans = metrics.silhouette_samples(dist,kmeans.labels_ , metric='precomputed')\nsilhouette_score = metrics.silhouette_score(dist, kmeans.labels_ , metric='precomputed')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "# Create a subplot with 1 row and 2 columns\nfig, ax = plt.subplots(1, 1)\nfig.set_size_inches(14, 7)\n\n\n # The silhouette coefficient can range from -1, to 1 \nax.set_xlim([-1, 1])\n# The (n_clusters+1)*10 is for inserting blank space between silhouette\n# plots of individual clusters, to demarcate them clearly.\nax.set_ylim([0, len(pre_cluster_sample) + (final_k + 1) * 10])\n\nlabels = final_kmeans.labels_\n\ny_lower = 10\n\nfor i in range(final_k):\n    # Aggregate the silhouette scores for samples belonging to\n    # cluster i, and sort them\n    ith_cluster_silhouette_values = silhouette_kmeans[labels == i]\n     \n    ith_cluster_silhouette_values.sort()\n\n    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n    y_upper = y_lower + size_cluster_i\n\n    color = cm.spectral(float(i) / final_k)\n    ax.fill_betweenx(np.arange(y_lower, y_upper),\n                        0, ith_cluster_silhouette_values,\n                        facecolor=color, edgecolor=color, alpha=0.7)\n\n    # Label the silhouette plots with their cluster numbers at the middle\n    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i+1))\n\n    # Compute the new y_lower for next plot\n    y_lower = y_upper + 10  # 10 for the 0 samples\n\n    \nax.set_title(\"Silhouette plot\", fontsize=14, fontweight='bold')\nax.set_xlabel(\"The silhouette coefficient values\", fontsize=14, fontweight='bold')\nax.set_ylabel(\"Cluster label\", fontsize=14, fontweight='bold')\n\n# The vertical line for average silhouette score of all the values\nax.axvline(x=silhouette_score, color=\"red\", linestyle=\"--\")\n\nax.set_yticks([])  # Clear the yaxis labels / ticks\nax.set_xticks([-0.4 ,-0.2, 0.0, 0.2, 0.4, 0.6, 0.8])\nfig.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "silhouette_score",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### External Validation \nFinding clusters which have differences in the variables we used to cluster is pretty much inevitable. The clusters are only relevent if there are differences in variables that were not included in the clustering. We have several catagorical variables in our original dataset that were not included in the analysis. Pick two or three variables that you think will be interesting to compare to see if they differ between clusters "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "outcome_factors = [] # Set the catagorical variables you want to examine here \noutcome_df = data_clean.loc[sample_idx ,outcome_factors] ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "outcome_df['cluster'] = 0\nfor i in range(final_k):\n    outcome_df['cluster'][final_kmeans.labels_ == i] = (i + 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "outcome_table = TableOne(outcome_df , outcome_factors, outcome_factors, groupby = 'cluster' ,pval = True  )\noutcome_table",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Did you find a significant difference between your clusters? \n\n#### Other methods of external validation \nTo test if the clusters you found are repeatable you can take another sample from the original data set, re-run the analysis and compare the results "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Conclusion \nHow would you characterise the clusters you found ?\n<br>\nDo you think K-Means found \"good\" clusters, why? \n<br>\nWould you say the clusters you found are useful or clinically relevent ? \n<br>\nWhat are the benefits of using this method ? \n<br>\nWhat are the problems of using this method and how would you try to counteract them ? "
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}